{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CASO PRÁCTICO 1: OPTIMIZACIÓN DE FLUJOS EN ALMACENES CON QLEARNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "#INICIALIZACIÓN DE LIBRERÍAS\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Configuración de parámetros gamma y alpha para el algoritmo de qlearning\n",
    "gamma = 0.75\n",
    "alpha = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PARTE 1 - DEFINICIÓN DEL ENTORNO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definición de los estados\n",
    "location_to_state = {'A':0,\n",
    "                     'B':1,\n",
    "                     'C':2,\n",
    "                     'D':3,\n",
    "                     'E':4,\n",
    "                     'F':5,\n",
    "                     'G':6,\n",
    "                     'H':7,\n",
    "                     'I':8,\n",
    "                     'J':9,\n",
    "                     'K':10,\n",
    "                     'L':11\n",
    "                    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definición de las acciones\n",
    "actions = list(range(0,12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definición de las recompensas\n",
    "R = np.array([[0,1,0,0,0,0,0,0,0,0,0,0], # A\n",
    "              [1,0,1,0,0,1,0,0,0,0,0,0], # B\n",
    "              [0,1,0,0,0,0,1,0,0,0,0,0], # C\n",
    "              [0,0,0,0,0,0,0,1,0,0,0,0], # D\n",
    "              [0,0,0,0,0,0,0,0,1,0,0,0], # E\n",
    "              [0,1,0,0,0,0,0,0,0,1,0,0], # F\n",
    "              [0,0,1,0,0,0,1,1,0,0,0,0], # G\n",
    "              [0,0,0,1,0,0,1,0,0,0,0,1], # H\n",
    "              [0,0,0,0,1,0,0,0,0,1,0,0], # I\n",
    "              [0,0,0,0,0,1,0,0,1,0,1,0], # J\n",
    "              [0,0,0,0,0,0,0,0,0,1,0,1], # K\n",
    "              [0,0,0,0,0,0,0,1,0,0,1,0]])# L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PARTE 2 - CONSTRUCCIÓN DE LA SOLUCIÓN DE IA CON Q-LEARNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementación del proceso de Q-learning\n",
    "def qlearning(R_new):\n",
    "    # Inicialización de los valores de Q\n",
    "    Q = np.zeros(shape = (12,12))\n",
    "    for i in range(1000):\n",
    "        current_state = np.random.randint(0,12) # estado actual s(t)\n",
    "        current_action = np.random.randint(0,12)\n",
    "        while R_new[current_state][current_action] < 1:\n",
    "            current_action = np.random.randint(0,12)\n",
    "        next_state = current_action\n",
    "        TD = R_new[current_state][current_action] + (gamma * Q[next_state][np.argmax(Q[next_state,])]) - Q[current_state][current_action]\n",
    "        Q[current_state][current_action] += alpha*TD\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PARTE 3 - PONER EL MODELO EN PRODUCCIÓN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_to_location = {y:x for x,y in location_to_state.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función que devuelve la ruta óptima\n",
    "\n",
    "def route(starting_location, ending_location):\n",
    "    R_new = np.copy(R)\n",
    "    ending_state = location_to_state[ending_location]\n",
    "    R_new[ending_state, ending_state] = 1000\n",
    "    Q = qlearning(R_new) # Llamamos a la función qlearning y le pasamos R_new para calcular Q\n",
    "    route = [starting_location]\n",
    "    next_location = starting_location\n",
    "    while next_location != ending_location:\n",
    "        starting_state = location_to_state[starting_location]\n",
    "        next_state = np.argmax(Q[starting_state,])\n",
    "        next_location = state_to_location[next_state]\n",
    "        route.append(next_location)\n",
    "        starting_location = next_location\n",
    "    return route"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['E', 'I', 'J', 'K', 'L', 'H', 'G']"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creamos la función para la ruta óptima\n",
    "def optimal_route(starting_location, first_stop, ending_location):\n",
    "    opt_route = route(starting_location, first_stop)[:-1] + route(first_stop, ending_location)\n",
    "    return opt_route\n",
    "# Imprimimos la ruta óptima\n",
    "optimal_route('E', 'K','G')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
